# Feature: 2.1.1 Basic Agent Module

**Branch**: `feature/2.1.1-basic-agent-module`
**Phase**: Phase 2 - LLM Agent with Chain-of-Thought Reasoning

## Problem Statement

JidoCode needs a core LLM agent that handles chat interactions with configurable model selection. This agent will be the foundation for all LLM-powered features in the TUI, enabling users to interact with various LLM providers through a unified interface.

## Solution Overview

Create `JidoCode.Agents.LLMAgent` module that wraps JidoAI's Agent system, providing:
- Dynamic model configuration from JidoCode.Config
- A coding assistant system prompt
- Chat functionality with PubSub broadcasting for TUI integration
- Proper supervision under AgentSupervisor

## Technical Details

### Dependencies
- `Jido.AI.Agent` - Core agent functionality
- `Jido.AI.Model` - Model configuration
- `JidoCode.Config` - LLM configuration
- `Phoenix.PubSub` - Event broadcasting

### Key APIs (from JidoAI research)
```elixir
# Start agent
Jido.AI.Agent.start_link(ai: [model: model, prompt: prompt])

# Chat
Jido.AI.Agent.chat_response(pid, message, timeout: timeout)

# Model creation
Jido.AI.Model.from({:provider, [model: "model-name", temperature: 0.7]})
```

### Files to Create/Modify
- `lib/jido_code/agents/llm_agent.ex` - New LLM agent module
- `test/jido_code/agents/llm_agent_test.exs` - Tests

## Implementation Plan

### Step 1: Create LLMAgent Module Structure
- [ ] Create `lib/jido_code/agents/llm_agent.ex`
- [ ] Define module with @moduledoc
- [ ] Add aliases for Jido.AI.Agent, Jido.AI.Model, JidoCode.Config

### Step 2: Implement start_link/1
- [ ] Accept options keyword list with optional overrides
- [ ] Build model from Config using `Jido.AI.Model.from/1`
- [ ] Define coding assistant system prompt
- [ ] Call `Jido.AI.Agent.start_link/1` with ai config

### Step 3: Implement chat/2
- [ ] Accept pid and message string
- [ ] Call `Jido.AI.Agent.chat_response/3`
- [ ] Broadcast response to PubSub "tui.events" topic
- [ ] Return response to caller

### Step 4: Add Helper Functions
- [ ] `get_config/0` - Return current model config
- [ ] `build_model/1` - Build model from provider config

### Step 5: Write Tests
- [ ] Test start_link with valid config
- [ ] Test start_link with missing config
- [ ] Test chat round-trip (will need mock or integration test approach)
- [ ] Test PubSub broadcast

### Step 6: Update Phase Plan
- [ ] Mark subtasks complete in phase-02.md
- [ ] Mark task 2.1.1 complete

## System Prompt Design

```
You are JidoCode, an expert coding assistant running in a terminal interface.

Your capabilities:
- Answer programming questions across all languages
- Explain code, algorithms, and concepts
- Help debug issues and suggest fixes
- Provide code examples and best practices
- Assist with architecture and design decisions

Guidelines:
- Be concise but thorough - terminal space is limited
- Use markdown code blocks with language hints
- When showing code changes, be specific about file locations
- Ask clarifying questions when requirements are ambiguous
- Acknowledge limitations when you're uncertain
```

## Success Criteria

1. LLMAgent can be started with config from JidoCode.Config
2. chat/2 sends message and receives response
3. Responses are broadcast to PubSub for TUI consumption
4. Tests pass with mock/integration approach

## Current Status

- [x] Step 1: Create LLMAgent Module Structure
- [x] Step 2: Implement start_link/1
- [x] Step 3: Implement chat/2
- [x] Step 4: Add Helper Functions
- [x] Step 5: Write Tests
- [x] Step 6: Update Phase Plan

## What Works

- LLMAgent starts with configuration from JidoCode.Config or explicit options
- Agent configuration can override base config values
- get_config/1 returns current model configuration
- chat/2 sends messages to AI agent and broadcasts responses to PubSub
- Agent handles linked AI agent exits with trap_exit
- Tests pass (6 unit tests + 1 skipped integration test)

## Files Created

- `lib/jido_code/agents/llm_agent.ex` - Main agent module
- `test/jido_code/agents/llm_agent_test.exs` - Tests

## Notes

- JidoAI uses EEx templates for prompt interpolation with `@message` variable
- Agent start_link returns `{:ok, pid}` which can be supervised
- chat_response/3 accepts timeout option (default 30_000ms)
- PubSub topic "tui.events" matches Phase 2 architecture for TUI integration
