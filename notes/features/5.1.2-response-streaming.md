# Feature: 5.1.2 Response Streaming

## Problem Statement

Currently, the LLM agent waits for the full response from the AI before displaying anything to the user. This creates a poor user experience, especially for long responses. Users see a "processing" state for extended periods with no feedback on progress.

## Solution Overview

Implement token-by-token streaming display showing LLM output as it arrives:

1. Configure LLMAgent to use streaming mode when calling JidoAI
2. LLMAgent broadcasts stream chunks via PubSub as they arrive
3. TUI appends chunks to a "streaming message" with a cursor indicator
4. On stream completion, finalize the message and clear streaming state

## Technical Details

### JidoAI Streaming Architecture

From research, JidoAI supports streaming via `ReqLLM.stream_text/3`:

```elixir
# In Jido.AI.Actions.ReqLlm.ChatCompletion
{:ok, stream} = Jido.AI.Actions.ReqLlm.ChatCompletion.run(%{
  model: model,
  prompt: prompt,
  stream: true
})

# Stream chunks have format:
%{
  content: "chunk text",
  finish_reason: nil | "stop" | "length",
  delta: %{content: "chunk text", role: "assistant"}
}
```

### Files to Modify

1. `lib/jido_code/agents/llm_agent.ex` - Add streaming chat function
2. `lib/jido_code/tui.ex` - Handle stream events and display

### Dependencies

- `Jido.AI.Actions.ReqLlm.ChatCompletion` - Chat action with stream: true
- `Phoenix.PubSub` - For broadcasting stream chunks
- `JidoCode.TUI` - For displaying streaming content

### Key Design Decisions

1. **New `chat_stream/3` function**: Add alongside existing `chat/3` to not break existing behavior
2. **PubSub events**: Use `{:stream_chunk, text}`, `{:stream_end, full_content}`, `{:stream_error, reason}`
3. **Streaming state in TUI Model**: Add `streaming_message` field to accumulate chunks
4. **Visual indicator**: Show blinking cursor `▌` during streaming
5. **Fallback**: If streaming fails, fall back to non-streaming response

## Implementation Plan

### Step 1: Add streaming support to LLMAgent

- [ ] 1.1 Add `chat_stream/3` function that calls agent with stream: true
- [ ] 1.2 Create stream handler GenServer callback
- [ ] 1.3 Enumerate stream chunks and broadcast via PubSub
- [ ] 1.4 Handle stream completion - broadcast `{:stream_end, full_content}`
- [ ] 1.5 Handle stream errors - broadcast `{:stream_error, reason}`

### Step 2: Update TUI Model for streaming

- [ ] 2.1 Add `streaming_message: String.t() | nil` to Model
- [ ] 2.2 Add `is_streaming: boolean()` to Model
- [ ] 2.3 Initialize both as nil/false

### Step 3: Implement TUI stream handlers

- [ ] 3.1 Handle `{:stream_chunk, text}` - append to streaming_message
- [ ] 3.2 Handle `{:stream_end, full_content}` - finalize message, clear streaming state
- [ ] 3.3 Handle `{:stream_error, reason}` - show error, clear streaming state
- [ ] 3.4 Update agent_status appropriately during streaming

### Step 4: Update TUI view for streaming

- [ ] 4.1 Display streaming_message with cursor indicator when streaming
- [ ] 4.2 Show streaming indicator in status bar (optional animated spinner)

### Step 5: Update dispatch_to_agent to use streaming

- [ ] 5.1 Change `LLMAgent.chat/3` call to `LLMAgent.chat_stream/3`
- [ ] 5.2 Update existing tests to accommodate streaming behavior

### Step 6: Write tests

- [ ] 6.1 Test stream_chunk appends to streaming_message
- [ ] 6.2 Test stream_end finalizes message and clears streaming state
- [ ] 6.3 Test stream_error shows error message
- [ ] 6.4 Test streaming indicator appears during streaming
- [ ] 6.5 Integration test with mock stream (optional, may require API key)

## Message Flow

```
User submits message
    ↓
TUI.dispatch_to_agent/2
    ↓
Task.start → LLMAgent.chat_stream/3
    ↓
State: agent_status = :processing, is_streaming = true
    ↓
LLMAgent calls ReqLLM.stream_text/3
    ↓
For each chunk in stream:
    LLMAgent broadcasts {:stream_chunk, text}
    TUI appends to streaming_message
    View re-renders with cursor
    ↓
On stream end:
    LLMAgent broadcasts {:stream_end, full_content}
    TUI adds final message, clears streaming_message
    State: agent_status = :idle, is_streaming = false
```

## PubSub Events

| Event | Data | Description |
|-------|------|-------------|
| `{:stream_chunk, text}` | String chunk | Partial response text |
| `{:stream_end, content}` | Full response | Stream completed successfully |
| `{:stream_error, reason}` | Error term | Stream failed |

## TUI Model Changes

```elixir
@type t :: %__MODULE__{
  # ... existing fields ...
  streaming_message: String.t() | nil,
  is_streaming: boolean()
}

defstruct # ... existing fields ...
  streaming_message: nil,
  is_streaming: false
```

## Success Criteria

1. User sees text appearing progressively as the LLM generates it
2. Blinking cursor indicates streaming is in progress
3. Status bar shows streaming state
4. Stream errors are handled gracefully with user-visible error messages
5. Non-streaming fallback works if streaming fails
6. All tests pass

## Notes

- JidoAI uses ReqLLM.stream_text/3 for streaming
- Stream chunks contain `content` and `delta.content` fields
- `finish_reason` is nil during streaming, then "stop" or "length" at end
- The Jido.AI.Agent doesn't have a streaming method yet - we'll need to implement streaming directly in LLMAgent using ChatCompletion action
